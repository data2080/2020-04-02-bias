---
layout: handouts
slideset: slides
permalink: handout/
---



# Welcome to class 9

The next two classes will be about bias and fairness. First we'll talk about how AI systems become biased. We will look at this through a variety of lenses to get a broad view. We will examine both case studies of popular cases and more academic audits. This week we'll focus on understanding bias. We'll get specific about what it means for an AI system to be biased and how AI systems produce biased decisions.


<hr>

# Readings

[Propublica Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) this is a classic article in the fair machine learning community. In class, we'll work with their data and reproduce their results.

[Fair ML Book Classification Chapter](https://fairmlbook.org/pdf/classification.pdf) This textbook chapter lays out how nondiscrimination can be defined. In reading, keep track of the various definitions that appear and the running examples. Skip over the problem set and read the last bit of the chapter.


<hr>


# Bias Audit Checklist

Summary: Find an AI system you're interested in. The system must have some potential for bias. Analyze its potential for bias and plan how you would evaluate the system to determine if it was biased or not.

As evidence of your analysis, submit a brief (up to 3 pages) report that answers (at least) the following questions.


1. Identify the system with link
1. Summarize what it does (input, output, etc)
1. What sources of bias might this system be susceptible to?
1. What questions would you ask about the data used for training?
1. What EDA would you do if you had access to the training data?
1. What type of audit would you recommend?
1. What data would be needed for an audit?
1. What type of fairness metric(s) would you recommend for this and why?

An example template is available [here](https://www.overleaf.com/read/bvqnwqyzvkhw), but any format that answers the questions is acceptable.
You may add any additional sections that you think are helpful.

You will be graded on:
- clear concise writing overall (5 points)
- sufficiently detailed, but concise description of the system (5 points)
- appropriate, thorough assessment of possible biases (10 points)
- appropriate plan for determining if the system is biased (5 points)
- identification and justification of fairness metric(s) for the problem (5 points)

Recent years' KDD Applied Data Science track might be a good place to choose a candidate system to evaluate. They are listed under a heading "Applied Data Science..." in each year's accepted papers.

- [2019](https://www.kdd.org/kdd2019/accepted-papers)
- [2018](https://www.kdd.org/kdd2018/accepted-papers)
- [2017](https://www.kdd.org/kdd2017/accepted-papers)
- [2016](https://www.kdd.org/kdd2016/program/accepted-papers)
<!--
target audience:
- team that built the system
- journalist pitch
- impacted person's lawyer -->

